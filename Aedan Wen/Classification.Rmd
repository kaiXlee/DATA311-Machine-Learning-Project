---
title: "Jobs"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

Initial Data Setup
```{r}

##Change Path of data
jobs <- read.csv('/Users/aedanwen/Downloads/jobs.csv')
##Were keeping job_disch and job_disc as numeric because its on a scale.
jobs$treat <- as.factor(jobs$treat)
jobs$comply <- as.factor(jobs$comply)
#Control is the same thing as treat
jobs$control <- NULL
#ID tag isnt needed
jobs$X <- NULL
##Were only classifying those who were selected for the program.
jobs <- jobs[!(jobs$treat == 0),]
#We dont need this anymore
jobs$treat <- NULL
jobs
set.seed(18271398)

observ_num = nrow(jobs)
trainindex <- sample(1:nrow(jobs), observ_num*0.75) 
train <- jobs[trainindex, ]
test <- jobs[-trainindex, ]
train
test
print(paste('Number of non-comply in test: ', length(test$comply[test$comply == 0])))
print(paste('Number of comply in test: ', length(test$comply[test$comply == 1])))
```

```{r}

```
After K-Fold cross validating at K = 10,20,50,100,450(LOOCV) it was found the the highest accuracy for the tree was at 2 or 3 nodes giving 56% accuracy. 

```{r}
library(tree)
library(caret)
library(MLmetrics)
tree.jobs <- tree(comply~., data = train)
###Tried K = 10,20,50,100 and LOOCV
#10 = 3 Node = 56.7% Acc
#20 = 3 Nodes = 56.7% Accuracy
#50 = 3 nodes at 56% Accuracy
#100 = 3 nodes at 56% Accuracy
#LOOCV(450) = 3 Nodes at 56% Accuracy
cv.tree.jobs <- cv.tree(tree.jobs,K = 450)

plot(cv.tree.jobs)
pr.tree.jobs <- prune.tree(tree.jobs,best = 4)

plot(pr.tree.jobs)
text(pr.tree.jobs, pretty = 0)

preds <- predict(tree.jobs,test,type='class')
confusionMatrix(preds,test$comply)
print(paste('Log Loss: ',LogLoss(as.numeric(preds),as.numeric(test$comply))))
```
This tree plot has the same 2 leaves on the left.

```{r}

library(gclus)
lda.jobs <- lda(comply~.,data = train)
preds <- predict(lda.jobs,test)$class
confusionMatrix(preds,test$comply)
print(paste('Log Loss: ',LogLoss(as.numeric(preds),as.numeric(test$comply))))
plot(lda.jobs)
```

```{r}
set.seed(1234)
library(randomForest)
?randomForest
for(i in 1:14){
rf.jobs <- randomForest(comply~.,data = train, mtry = i )
preds <- predict(rf.jobs,test,type = 'class')
print(paste('Accuracy: ' ,sum(preds == test$comply)/length(preds), '  -- mtry = ',i))
}

```
Best results come from mtry= 13/14 or 3/4
```{r}
set.seed(1234)
rf.jobs <- randomForest(comply~.,data = train, mtry = 13)
preds <- predict(rf.jobs,test,type = 'class')
confusionMatrix(preds,test$comply)
print(paste('Log Loss: ',LogLoss(as.numeric(preds),as.numeric(test$comply))))
plot(rf.jobs)
```

